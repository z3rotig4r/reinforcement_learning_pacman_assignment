# Project 2: Reinforcement Learning

**CSE 17182 Artificial Intelligence, Fall 2025**

ì´ í”„ë¡œì íŠ¸ëŠ” UC Berkeleyì˜ Pacman AI í”„ë¡œì íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°•í™”í•™ìŠµ(Reinforcement Learning) ê³¼ì œì…ë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

Value Iterationê³¼ Q-Learning ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•˜ì—¬ Gridworld, Crawler Robot, Pacman í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.

## ğŸ¯ ìµœì¢… ì ìˆ˜: 21/21 (ë§Œì )

| ë¬¸ì œ | ë‚´ìš© | ë°°ì  | íšë“ |
|------|------|------|------|
| Q1 | Value Iteration | 5ì  | âœ… 5/5 |
| Q2 | Bridge Crossing Analysis | 5ì  | âœ… 5/5 |
| Q3 | Q-Learning | 5ì  | âœ… 5/5 |
| Q4 | Epsilon Greedy | 2ì  | âœ… 2/2 |
| Q5 | Q-Learning and Pacman | 1ì  | âœ… 1/1 |
| Q6 | Approximate Q-Learning | 3ì  | âœ… 3/3 |

---

## ğŸ”§ êµ¬í˜„ ë‚´ìš©

### Q1: Value Iteration (`valueIterationAgents.py`)

MDP(Markov Decision Process)ì—ì„œ Value Iteration ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

**Value Iteration ì—…ë°ì´íŠ¸ ìˆ˜ì‹:**
$$V_{k+1}(s) \leftarrow \max_{a} \sum_{s'} T(s,a,s')[R(s,a,s') + \gamma V_k(s')]$$

#### êµ¬í˜„í•œ ë©”ì„œë“œ:
- **`runValueIteration()`**: ì§€ì •ëœ ë°˜ë³µ íšŸìˆ˜ë§Œí¼ value iteration ìˆ˜í–‰
- **`computeQValueFromValues(state, action)`**: ì£¼ì–´ì§„ ìƒíƒœ-í–‰ë™ ìŒì˜ Q-value ê³„ì‚°
- **`computeActionFromValues(state)`**: í˜„ì¬ value function ê¸°ë°˜ ìµœì  í–‰ë™ ë°˜í™˜

```python
# Q-value ê³„ì‚°
Q(s,a) = Î£ T(s,a,s') * [R(s,a,s') + Î³ * V(s')]
```

---

### Q2: Policies (`analysis.py`)

DiscountGridì—ì„œ ë‹¤ì–‘í•œ ì •ì±…ì„ ìœ ë„í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°(discount, noise, livingReward) ì„¤ì •:

| ë¬¸ì œ | ëª©í‘œ ì •ì±… | discount | noise | livingReward |
|------|----------|----------|-------|--------------|
| 2a | ê°€ê¹Œìš´ ì¶œêµ¬(+1), ì ˆë²½ ìœ„í—˜ ê°ìˆ˜ | 0.2 | 0.0 | 0.0 |
| 2b | ê°€ê¹Œìš´ ì¶œêµ¬(+1), ì ˆë²½ íšŒí”¼ | 0.2 | 0.2 | 0.0 |
| 2c | ë¨¼ ì¶œêµ¬(+10), ì ˆë²½ ìœ„í—˜ ê°ìˆ˜ | 0.9 | 0.0 | 0.0 |
| 2d | ë¨¼ ì¶œêµ¬(+10), ì ˆë²½ íšŒí”¼ | 0.9 | 0.2 | 0.0 |
| 2e | ëª¨ë“  ì¶œêµ¬/ì ˆë²½ íšŒí”¼ (ì˜ì›íˆ ìƒì¡´) | 0.9 | 0.2 | 11 |

**íŒŒë¼ë¯¸í„° ì„ íƒ ì›ë¦¬:**
- **discount**: ë‚®ìœ¼ë©´ ê°€ê¹Œìš´ ë³´ìƒ ì„ í˜¸, ë†’ìœ¼ë©´ ë¨¼ ë³´ìƒë„ ê³ ë ¤
- **noise**: ë†’ìœ¼ë©´ ì ˆë²½ ê·¼ì²˜ê°€ ìœ„í—˜í•´ì ¸ íšŒí”¼
- **livingReward**: ë†’ìœ¼ë©´ ì¢…ë£Œ ìƒíƒœë¥¼ í”¼í•˜ê³  ê³„ì† ì‚´ì•„ìˆìœ¼ë ¤ í•¨

---

### Q3: Q-Learning (`qlearningAgents.py`)

ëª¨ë¸ ì—†ì´ ê²½í—˜ì„ í†µí•´ í•™ìŠµí•˜ëŠ” Q-Learning ì—ì´ì „íŠ¸ êµ¬í˜„:

**Q-Learning ì—…ë°ì´íŠ¸ ìˆ˜ì‹:**
$$Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

#### êµ¬í˜„í•œ ë©”ì„œë“œ:
- **`__init__()`**: Q-values ì €ì¥ì„ ìœ„í•œ `util.Counter()` ì´ˆê¸°í™”
- **`getQValue(state, action)`**: Q(s,a) ê°’ ë°˜í™˜ (ë¯¸ë°©ë¬¸ ìƒíƒœëŠ” 0.0)
- **`computeValueFromQValues(state)`**: $V(s) = \max_a Q(s,a)$
- **`computeActionFromQValues(state)`**: ìµœì  í–‰ë™ ì„ íƒ (ë™ì  ì‹œ ëœë¤ ì„ íƒ)
- **`update(state, action, nextState, reward)`**: Q-value ì—…ë°ì´íŠ¸

---

### Q4: Epsilon Greedy (`qlearningAgents.py`)

íƒí—˜(Exploration)ê³¼ í™œìš©(Exploitation)ì˜ ê· í˜•ì„ ìœ„í•œ Îµ-greedy ì •ì±…:

```python
def getAction(state):
    if random() < epsilon:
        return random_action()  # íƒí—˜: ëœë¤ í–‰ë™
    else:
        return best_action()    # í™œìš©: ìµœì  í–‰ë™
```

- í™•ë¥  Îµë¡œ ëœë¤ í–‰ë™ ì„ íƒ (íƒí—˜)
- í™•ë¥  1-Îµë¡œ í˜„ì¬ ìµœì  í–‰ë™ ì„ íƒ (í™œìš©)

---

### Q5: Q-Learning and Pacman

Q3, Q4ì—ì„œ êµ¬í˜„í•œ Q-Learning ì—ì´ì „íŠ¸ê°€ Pacman ê²Œì„ì—ì„œë„ ë™ì‘í•˜ëŠ”ì§€ ê²€ì¦:

```bash
python pacman.py -p PacmanQAgent -x 2000 -n 2010 -l smallGrid
```

- 2000ë²ˆ í•™ìŠµ í›„ 100ë²ˆ í…ŒìŠ¤íŠ¸ì—ì„œ **100% ìŠ¹ë¥ ** ë‹¬ì„±

---

### Q6: Approximate Q-Learning (`qlearningAgents.py`)

Feature ê¸°ë°˜ Q-Learningìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ:

**Approximate Q-function:**
$$Q(s,a) = \sum_{i=1}^{n} w_i \cdot f_i(s,a)$$

**Weight ì—…ë°ì´íŠ¸:**
$$w_i \leftarrow w_i + \alpha \cdot \text{difference} \cdot f_i(s,a)$$
$$\text{difference} = [r + \gamma \max_{a'} Q(s',a')] - Q(s,a)$$

#### êµ¬í˜„í•œ ë©”ì„œë“œ:
- **`getQValue(state, action)`**: feature vectorì™€ weightì˜ ë‚´ì ìœ¼ë¡œ Q-value ê³„ì‚°
- **`update(state, action, nextState, reward)`**: TD errorë¥¼ ì´ìš©í•œ weight ì—…ë°ì´íŠ¸

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### í™˜ê²½ ì„¤ì •
```bash
conda create -n pacman python=3.9 -y
conda activate pacman
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
python autograder.py

# íŠ¹ì • ë¬¸ì œë§Œ í…ŒìŠ¤íŠ¸
python autograder.py -q q1
python autograder.py -q q2
# ...
```

### Gridworld ì‹¤í–‰
```bash
# ìˆ˜ë™ ì œì–´
python gridworld.py -m

# Value Iteration ì—ì´ì „íŠ¸
python gridworld.py -a value -i 100 -k 10

# Q-Learning ì—ì´ì „íŠ¸
python gridworld.py -a q -k 100
```

### Pacman ì‹¤í–‰
```bash
# Q-Learning Pacman
python pacman.py -p PacmanQAgent -x 2000 -n 2010 -l smallGrid

# Approximate Q-Learning Pacman
python pacman.py -p ApproximateQAgent -a extractor=SimpleExtractor -x 50 -n 60 -l mediumGrid
```

### Crawler Robot ì‹¤í–‰
```bash
python crawler.py
```

---

## ğŸ“ ìˆ˜ì •ëœ íŒŒì¼

ê³¼ì œ ì§€ì¹¨ì— ë”°ë¼ ë‹¤ìŒ 3ê°œ íŒŒì¼ë§Œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤:

1. **`valueIterationAgents.py`** - Value Iteration ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
2. **`qlearningAgents.py`** - Q-Learning ë° Approximate Q-Learning êµ¬í˜„
3. **`analysis.py`** - Bridge Crossing ë¶„ì„ íŒŒë¼ë¯¸í„° ì„¤ì •

---

## ğŸ“š ì°¸ê³  íŒŒì¼

- `mdp.py` - MDP ì¸í„°í˜ì´ìŠ¤ ì •ì˜
- `learningAgents.py` - ì—ì´ì „íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤
- `util.py` - ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Counter ë“±)
- `gridworld.py` - Gridworld í™˜ê²½
- `featureExtractors.py` - Approximate Q-Learningìš© feature ì¶”ì¶œê¸°

---

## ğŸ”‘ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ìš”ì•½

### Value Iteration vs Q-Learning

| êµ¬ë¶„ | Value Iteration | Q-Learning |
|------|-----------------|------------|
| ìœ í˜• | Model-based (ê³„íš) | Model-free (í•™ìŠµ) |
| MDP í•„ìš” | âœ… ì „ì´ í™•ë¥  í•„ìš” | âŒ ê²½í—˜ìœ¼ë¡œ í•™ìŠµ |
| ìˆ˜ë ´ | ë°˜ë³µ íšŸìˆ˜ë¡œ ë³´ì¥ | ì¶©ë¶„í•œ íƒí—˜ í•„ìš” |
| ì ìš© | ì‘ì€ ìƒíƒœ ê³µê°„ | ëŒ€ê·œëª¨ í™˜ê²½ ê°€ëŠ¥ |

### Approximate Q-Learning ì¥ì 

- **ì¼ë°˜í™”**: ë¹„ìŠ·í•œ ìƒíƒœë“¤ì´ ê°™ì€ featureë¥¼ ê³µìœ 
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ëª¨ë“  (s,a) ìŒ ëŒ€ì‹  weightë§Œ ì €ì¥
- **ëŒ€ê·œëª¨ í™˜ê²½**: Pacman ê°™ì€ í° ìƒíƒœ ê³µê°„ì—ì„œë„ í•™ìŠµ ê°€ëŠ¥

